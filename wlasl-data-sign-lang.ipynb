{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2632847,"sourceType":"datasetVersion","datasetId":1589971},{"sourceId":4747505,"sourceType":"datasetVersion","datasetId":2747345}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mediapipe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:25:17.023790Z","iopub.execute_input":"2025-10-19T14:25:17.024088Z","iopub.status.idle":"2025-10-19T14:25:49.314642Z","shell.execute_reply.started":"2025-10-19T14:25:17.024062Z","shell.execute_reply":"2025-10-19T14:25:49.313308Z"}},"outputs":[{"name":"stdout","text":"Collecting mediapipe\n  Downloading mediapipe-0.10.21-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (25.1.0)\nRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\nRequirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\nRequirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.5)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\nRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\nCollecting protobuf<5,>=4.25.3 (from mediapipe)\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nCollecting sounddevice>=0.4.4 (from mediapipe)\n  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (2.4.1)\nRequirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\nRequirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\nRequirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2->mediapipe) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2->mediapipe) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2->mediapipe) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2->mediapipe) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2->mediapipe) (2024.2.0)\nDownloading mediapipe-0.10.21-cp310-cp310-manylinux_2_28_x86_64.whl (35.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\nInstalling collected packages: protobuf, sounddevice, mediapipe\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.8 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed mediapipe-0.10.21 protobuf-4.25.8 sounddevice-0.5.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\nimport mediapipe as mp\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:25:49.319385Z","iopub.execute_input":"2025-10-19T14:25:49.319725Z","iopub.status.idle":"2025-10-19T14:26:07.683381Z","shell.execute_reply.started":"2025-10-19T14:25:49.319696Z","shell.execute_reply":"2025-10-19T14:26:07.681702Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"RAW_DATA_DIR = \"/kaggle/input/wlasl-processed\"\nPROCESSED_DATA_DIR = \"/kaggle/working\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:27:04.356639Z","iopub.execute_input":"2025-10-19T14:27:04.357509Z","iopub.status.idle":"2025-10-19T14:27:04.363192Z","shell.execute_reply.started":"2025-10-19T14:27:04.357459Z","shell.execute_reply":"2025-10-19T14:27:04.361814Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def euclidean_distance(v1, v2):\n    return np.sqrt((float(v1[0]) - float(v2[0])) ** 2 + (float(v1[1]) - float(v2[1])) ** 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:27:09.366900Z","iopub.execute_input":"2025-10-19T14:27:09.367296Z","iopub.status.idle":"2025-10-19T14:27:09.373184Z","shell.execute_reply.started":"2025-10-19T14:27:09.367268Z","shell.execute_reply":"2025-10-19T14:27:09.371553Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def ReadVideo(video_path):    \n    # Read the input video\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise ValueError(f\"Could not open video file {video_path}\")\n    return cap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:27:25.010926Z","iopub.execute_input":"2025-10-19T14:27:25.011441Z","iopub.status.idle":"2025-10-19T14:27:25.016652Z","shell.execute_reply.started":"2025-10-19T14:27:25.011407Z","shell.execute_reply":"2025-10-19T14:27:25.015395Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def getGrayFramesAndFrames(cap):\n    frames = []\n    gray_frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        frames.append(frame)        \n        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        gray_frames.append(gray_frame)\n    \n    cap.release()\n    return gray_frames, frames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:27:40.026462Z","iopub.execute_input":"2025-10-19T14:27:40.026800Z","iopub.status.idle":"2025-10-19T14:27:40.032576Z","shell.execute_reply.started":"2025-10-19T14:27:40.026774Z","shell.execute_reply":"2025-10-19T14:27:40.031341Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def calculate_histogram_differences(gray_frames):\n    HDiffs = []\n    for i in range(0, len(gray_frames)-1):\n        if (i == 0):\n            hist_curr = cv2.calcHist([gray_frames[i]], [0], None, [256], [0, 256])\n            continue\n        hist_prev = hist_curr\n        hist_curr = cv2.calcHist([gray_frames[i]], [0], None, [256], [0, 256])\n\n        Hdiff = np.sum(np.abs(hist_prev - hist_curr))\n        HDiffs.append(Hdiff)\n    return HDiffs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:27:53.679939Z","iopub.execute_input":"2025-10-19T14:27:53.680343Z","iopub.status.idle":"2025-10-19T14:27:53.687027Z","shell.execute_reply.started":"2025-10-19T14:27:53.680311Z","shell.execute_reply":"2025-10-19T14:27:53.685590Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def Extract_key_frames(video_path):\n    cap = ReadVideo(video_path)\n    gray_frames, frames = getGrayFramesAndFrames(cap)\n    HDiffs = calculate_histogram_differences(gray_frames)\n    mean = np.mean(HDiffs)\n    std = np.std(HDiffs)\n    threshold = mean + 0.5 * std\n    keyframes = []\n    for i in range(len(HDiffs)):\n        if HDiffs[i] > threshold:\n            keyframes.append(frames[i+1])\n    return keyframes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:28:03.867487Z","iopub.execute_input":"2025-10-19T14:28:03.867875Z","iopub.status.idle":"2025-10-19T14:28:03.874084Z","shell.execute_reply.started":"2025-10-19T14:28:03.867845Z","shell.execute_reply":"2025-10-19T14:28:03.872623Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def extract_frames(video_path):\n    cap = ReadVideo(video_path)\n    frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n\n    cap.release()\n    if len(frames) > 2:\n        return frames[1:-1]\n    else:\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:28:07.289996Z","iopub.execute_input":"2025-10-19T14:28:07.290524Z","iopub.status.idle":"2025-10-19T14:28:07.297006Z","shell.execute_reply.started":"2025-10-19T14:28:07.290491Z","shell.execute_reply":"2025-10-19T14:28:07.295579Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def extract_pose_landmarks(rgb_frame, mp_pose):\n    pose_results = mp_pose.process(rgb_frame)\n    pose_landmarks = []\n\n    if pose_results.pose_landmarks:\n        for i, lm in enumerate(pose_results.pose_landmarks.landmark):\n            if i < 17 and i not in [7, 8]:\n                pose_landmarks.append((lm.x, lm.y, lm.z))\n\n    return pose_landmarks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:28:21.697683Z","iopub.execute_input":"2025-10-19T14:28:21.698065Z","iopub.status.idle":"2025-10-19T14:28:21.704378Z","shell.execute_reply.started":"2025-10-19T14:28:21.698034Z","shell.execute_reply":"2025-10-19T14:28:21.703018Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def classify_hands (pose_landmarks, hand_landmarks):\n    left_wrist_pose  = pose_landmarks[13]\n    right_wrist_pose = pose_landmarks[14]  \n    wrist = hand_landmarks[0] \n    \n\n    dleft = euclidean_distance(left_wrist_pose, wrist)\n    dright = euclidean_distance(right_wrist_pose, wrist)\n   \n    if(dleft < dright):\n        return \"Left\"\n    else:\n        return \"Right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:28:37.968346Z","iopub.execute_input":"2025-10-19T14:28:37.968711Z","iopub.status.idle":"2025-10-19T14:28:37.975578Z","shell.execute_reply.started":"2025-10-19T14:28:37.968686Z","shell.execute_reply":"2025-10-19T14:28:37.974101Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def extract_hand_landmarks(rgb_frame, mp_hands, pose_landmarks):\n    hands_results = mp_hands.process(rgb_frame)\n    left_hand_landmarks = []\n    right_hand_landmarks = []\n\n    if hands_results.multi_hand_landmarks and hands_results.multi_handedness:\n        for hand_landmarks, handedness in zip(hands_results.multi_hand_landmarks, hands_results.multi_handedness):\n            # Không sử dụng hướng tay của mediapipe vì độ chính xác thấp và thường ngược hướng\n            landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n            if classify_hands(pose_landmarks, landmarks) == \"Right\":\n                right_hand_landmarks = landmarks\n            else:\n                left_hand_landmarks = landmarks   \n    \n    return left_hand_landmarks, right_hand_landmarks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:28:53.302505Z","iopub.execute_input":"2025-10-19T14:28:53.302871Z","iopub.status.idle":"2025-10-19T14:28:53.310216Z","shell.execute_reply.started":"2025-10-19T14:28:53.302842Z","shell.execute_reply":"2025-10-19T14:28:53.308400Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def extract_landmarks(frames):\n    mp_pose = mp.solutions.pose.Pose(static_image_mode=True)\n    mp_hands = mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.1)\n    \n    landmarks_dict = {}\n    \n    for idx, frame in enumerate(frames):\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        \n        pose_landmarks = extract_pose_landmarks(rgb_frame, mp_pose)\n        left_hand_landmarks, right_hand_landmarks = extract_hand_landmarks(rgb_frame, mp_hands ,pose_landmarks)\n        \n        landmarks_dict[idx] = {\n            \"pose\": pose_landmarks,\n            \"left\": left_hand_landmarks,\n            \"right\": right_hand_landmarks\n        }\n    mp_pose.close()\n    mp_hands.close()\n    return landmarks_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:29:01.934740Z","iopub.execute_input":"2025-10-19T14:29:01.935107Z","iopub.status.idle":"2025-10-19T14:29:01.942534Z","shell.execute_reply.started":"2025-10-19T14:29:01.935075Z","shell.execute_reply":"2025-10-19T14:29:01.940953Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# def filter_invalid_landmarks(landmarks_dict):\n#     validated = {}\n#     for landmarks_idx, landmarks_data in landmarks_dict.items():\n#         validated_landmarks = {}\n#         # Nếu không có pose thì bỏ qua frame\n#         if  not landmarks_data[\"pose\"]:\n#             continue\n#         for part in [\"pose\", \"right\", \"left\"]:\n#             # Nếu không có dữ liệu cho phần này, gán mặc định\n#             if part not in landmarks_data or not landmarks_data[part]:\n#                 if part in [\"right\", \"left\"]:\n#                     validated_landmarks[part] = [(0.0, 0.0)] * 21\n#             else:\n#                 processed_points = []\n#                 for point in landmarks_data[part]:\n#                     x = float(point[0])\n#                     y = float(point[1])\n#                     if not (0.0 <= x <= 1.0 and 0.0 <= y <= 1.0):\n#                         x, y = 0.0, 0.0\n#                     processed_points.append((x, y))\n#                 validated_landmarks[part] = processed_points\n#         validated[landmarks_idx] = validated_landmarks\n#     return validated\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:40:21.292096Z","iopub.execute_input":"2025-04-17T23:40:21.292504Z","iopub.status.idle":"2025-04-17T23:40:21.322302Z","shell.execute_reply.started":"2025-04-17T23:40:21.292464Z","shell.execute_reply":"2025-04-17T23:40:21.320805Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Cập nhật mới** Lọc và nội suy","metadata":{}},{"cell_type":"code","source":"def filter_and_interpolate_landmarks(landmarks_dict):\n    validated = {}\n\n    for i, (frame_idx, landmarks_data) in enumerate(landmarks_dict.items()):\n        validated_landmarks = {}\n\n        if not landmarks_data[\"pose\"]:\n            continue\n\n        for part in [\"pose\", \"right\", \"left\"]:\n            current = landmarks_data.get(part, [])\n\n            if part in [\"right\", \"left\"] and not current:\n                if 0 < i < len(landmarks_dict) - 1:\n                    prev_data = landmarks_dict.get(list(landmarks_dict.keys())[i - 1], {})\n                    next_data = landmarks_dict.get(list(landmarks_dict.keys())[i + 1], {})\n                    prev_points = prev_data.get(part, [])\n                    next_points = next_data.get(part, [])\n\n                    if prev_points and next_points:\n                        if not (all(p[0] == 0.0 and p[1] == 0.0 and p[2] == 0.0 for p in prev_points) or\n                                all(p[0] == 0.0 and p[1] == 0.0 and p[2] == 0.0 for p in next_points)):\n                            # Thực hiện nội suy nếu có dữ liệu hợp lệ\n                            interpolated_points = []\n                            for j in range(21):  # 21 points cho hand\n                                if j < len(prev_points) and j < len(next_points):\n                                    x = (prev_points[j][0] + next_points[j][0]) / 2\n                                    y = (prev_points[j][1] + next_points[j][1]) / 2\n                                    z = (prev_points[j][2] + next_points[j][2]) / 2\n                                    interpolated_points.append((x, y, z))\n                                else:\n                                    interpolated_points.append((0.0, 0.0, 0.0))\n                            validated_landmarks[part] = interpolated_points\n                            continue\n\n                validated_landmarks[part] = [(0.0, 0.0, 0.0)] * 21\n            else:\n                processed_points = []\n                for point in current:\n                    x = float(point[0])\n                    y = float(point[1])\n                    z = float(point[2])\n                    if not (0.0 <= x <= 1.0 and 0.0 <= y <= 1.0):\n                        x, y, z = 0.0, 0.0, 0.0\n                    processed_points.append((x, y, z))\n                validated_landmarks[part] = processed_points\n\n        validated[frame_idx] = validated_landmarks\n\n    return validated\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:29:30.026351Z","iopub.execute_input":"2025-10-19T14:29:30.026715Z","iopub.status.idle":"2025-10-19T14:29:30.042372Z","shell.execute_reply.started":"2025-10-19T14:29:30.026687Z","shell.execute_reply":"2025-10-19T14:29:30.040336Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"<h3> Hàm xuất ảnh đã gán landmarks cho kiểm thử","metadata":{}},{"cell_type":"code","source":"CUSTOM_POSE_CONNECTIONS = [\n    (0, 1), (0, 4), \n    (4, 5), (5, 6),         \n    (1, 2), (2, 3),         \n    (7, 8),                \n    (9, 10),               \n    (10, 12), (12, 14),    \n    (9, 11), (11, 13),     \n]\n\ndef draw_landmarks(frame, landmarks_dict):\n    h, w, _ = frame.shape  \n    annotated_frame = frame.copy()\n\n    pose_landmarks = landmarks_dict.get(\"pose\", [])\n    left_hand_landmarks = landmarks_dict.get(\"left\", [])\n    right_hand_landmarks = landmarks_dict.get(\"right\", [])\n\n    # Vẽ đường nối Pose (sử dụng kết nối tùy chỉnh)\n    if pose_landmarks and len(pose_landmarks) > 0:\n        for idx1, idx2 in CUSTOM_POSE_CONNECTIONS:\n            if idx1 < len(pose_landmarks) and idx2 < len(pose_landmarks):\n                x1, y1 , z1= pose_landmarks[idx1]\n                x2, y2 , z2= pose_landmarks[idx2]\n                if x1 and y1 and x2 and y2:\n                    x1, y1 = int(x1 * w), int(y1 * h)\n                    x2, y2 = int(x2 * w), int(y2 * h)\n                    cv2.line(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 5)\n\n    mp_hands = mp.solutions.hands\n    hand_connections = list(mp_hands.HAND_CONNECTIONS)\n    for hand, landmarks, color in [(\"left\", left_hand_landmarks, (255, 0, 0)), (\"right\", right_hand_landmarks, (0, 0, 255))]:\n        if landmarks and len(landmarks) > 0:\n            for idx1, idx2 in hand_connections:\n                if idx1 < len(landmarks) and idx2 < len(landmarks):\n                    x1, y1, z1 = landmarks[idx1]\n                    x2, y2, z2 = landmarks[idx2]\n                    if x1 and y1 and x2 and y2:\n                        x1, y1 = int(x1 * w), int(y1 * h)\n                        x2, y2 = int(x2 * w), int(y2 * h)\n                        cv2.line(annotated_frame, (x1, y1), (x2, y2), color, 2)\n\n    for part, landmarks, color in zip([\"pose\", \"left\", \"right\"], \n                                      [pose_landmarks, left_hand_landmarks, right_hand_landmarks], \n                                      [(0, 255, 0), (255, 0, 0), (0, 0, 255)]):\n        if landmarks and len(landmarks) > 0:\n            for x, y, z in landmarks:\n                if x and y:\n                    x, y = int(x * w), int(y * h)\n                    cv2.circle(annotated_frame, (x, y), 5, color, -1)\n\n    return annotated_frame\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:30:27.492463Z","iopub.execute_input":"2025-10-19T14:30:27.492803Z","iopub.status.idle":"2025-10-19T14:30:27.506931Z","shell.execute_reply.started":"2025-10-19T14:30:27.492777Z","shell.execute_reply":"2025-10-19T14:30:27.505789Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def rotate_landmarks(landmarks, angle_deg, center):\n    transformed = {}\n    theta = np.radians(angle_deg)\n    cos_theta = np.cos(theta)\n    sin_theta = np.sin(theta)\n    cx, cy = center\n    \n    for key in landmarks:\n        if landmarks[key]:\n            points = np.array(landmarks[key], dtype=np.float32)\n            transformed_points = []\n            \n            if len(points) == 0:\n                continue\n            z_values = points[:, 2]\n            max_z = np.max(z_values)\n            z_noise = np.random.choice([-1, 1]) * np.random.uniform(0.02, 0.04) * max_z\n\n            for x, y, z in points:\n                x_new = cx + (x - cx) * cos_theta - (y - cy) * sin_theta\n                y_new = cy + (x - cx) * sin_theta + (y - cy) * cos_theta\n                z_new = z + z_noise\n                transformed_points.append([x_new, y_new, z_new])\n            transformed[key] = transformed_points\n        else:\n            transformed[key] = []\n    return transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:30:46.915664Z","iopub.execute_input":"2025-10-19T14:30:46.916011Z","iopub.status.idle":"2025-10-19T14:30:46.923830Z","shell.execute_reply.started":"2025-10-19T14:30:46.915986Z","shell.execute_reply":"2025-10-19T14:30:46.922719Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def translate_landmarks(landmarks, tx, ty):\n    transformed = {}\n    for key in landmarks:\n        if landmarks[key]:\n            points = np.array(landmarks[key], dtype=np.float32)\n            transformed_points = []\n\n            if len(points) == 0:\n                continue\n            z_values = points[:, 2]\n            max_z = np.max(z_values)\n            z_noise = np.random.choice([-1, 1]) * np.random.uniform(0.02, 0.04) * max_z\n            for x, y, z in points:\n                x_new = x + tx\n                y_new = y + ty\n                z_new = z + z_noise\n                transformed_points.append([x_new, y_new, z_new])\n            transformed[key] = transformed_points\n        else:\n            transformed[key] = []\n    return transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:30:53.625958Z","iopub.execute_input":"2025-10-19T14:30:53.626362Z","iopub.status.idle":"2025-10-19T14:30:53.633982Z","shell.execute_reply.started":"2025-10-19T14:30:53.626331Z","shell.execute_reply":"2025-10-19T14:30:53.632777Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def squeeze_landmarks(landmarks, squeeze_x, squeeze_y):\n    transformed = {}\n    for key in landmarks:\n        if landmarks[key]:\n            points = np.array(landmarks[key], dtype=np.float32)\n            squeezed_points = []\n            \n            if len(points) == 0:\n                continue\n            z_values = points[:, 2]\n            max_z = np.max(z_values)\n            z_noise = np.random.choice([-1, 1]) * np.random.uniform(0.02, 0.04) * max_z\n            \n            for x, y, z in points:\n                x_new = x * squeeze_x\n                y_new = y * squeeze_y\n                z_new = z + z_noise\n                squeezed_points.append([x_new, y_new, z_new])\n            transformed[key] = squeezed_points\n        else:\n            transformed[key] = []\n    return transformed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:31:07.023526Z","iopub.execute_input":"2025-10-19T14:31:07.023892Z","iopub.status.idle":"2025-10-19T14:31:07.031950Z","shell.execute_reply.started":"2025-10-19T14:31:07.023864Z","shell.execute_reply":"2025-10-19T14:31:07.030015Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def generate_pts2(max_shift=0.15):\n    top_left = [np.random.uniform(0, max_shift), np.random.uniform(0, max_shift)]\n    top_right = [1 - np.random.uniform(0, max_shift), np.random.uniform(0, max_shift)]\n    bottom_right = [1 - np.random.uniform(0, max_shift), 1 - np.random.uniform(0, max_shift)]\n    bottom_left = [np.random.uniform(0, max_shift), 1 - np.random.uniform(0, max_shift)]\n\n    return np.float32([top_left, top_right, bottom_right, bottom_left])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:31:17.616418Z","iopub.execute_input":"2025-10-19T14:31:17.616747Z","iopub.status.idle":"2025-10-19T14:31:17.623317Z","shell.execute_reply.started":"2025-10-19T14:31:17.616724Z","shell.execute_reply":"2025-10-19T14:31:17.621980Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def perspective_transform(landmarks, pts2):\n    pts1 = np.float32([[0, 0], [1, 0], [1, 1], [0, 1]])\n    \n    M = cv2.getPerspectiveTransform(pts1, pts2)\n    \n    transformed = {}\n    for key in landmarks:\n        if landmarks[key]:\n            points = np.array(landmarks[key], dtype=np.float32)\n\n            if len(points) == 0:\n                continue\n            \n            xy_points = points[:, :2]    \n\n            transformed_points = cv2.perspectiveTransform(xy_points.reshape(-1, 1, 2), M).reshape(-1, 2)\n            \n            z_values = points[:, 2]\n            max_z = np.max(z_values)\n            z_noise = np.random.choice([-1, 1]) * np.random.uniform(0.02, 0.04) * max_z\n            \n            new_transformed_points = []\n            for i in range(len(transformed_points)):\n                transformed_point_with_z = np.append(transformed_points[i], z_values[i] + z_noise)\n                new_transformed_points.append(transformed_point_with_z)\n\n            transformed[key] = np.array(new_transformed_points).tolist()\n        else:\n            transformed[key] = []\n    return transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:31:46.711265Z","iopub.execute_input":"2025-10-19T14:31:46.711643Z","iopub.status.idle":"2025-10-19T14:31:46.720828Z","shell.execute_reply.started":"2025-10-19T14:31:46.711612Z","shell.execute_reply":"2025-10-19T14:31:46.719068Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def augment_data(landmarks_dict):\n    import numpy as np\n\n    rotated_landmarks_dict = {}\n    squeezed_landmarks_dict = {} \n    perspectived_landmarks_dict = {}\n    rotated_squeezed_landmarks_dict = {}\n    rotated_translated_landmarks_dict = {}\n    rotated_perspective_landmarks_dict = {}\n    translated_squeezed_landmarks_dict = {}\n    translated_perspective_landmarks_dict = {}\n    perspective_squeezed_landmarks_dict = {}\n\n    center = (0.5, 0.5)\n\n    angle_r = np.random.choice([-1, 1]) * np.random.uniform(10, 20)\n\n    squeeze_x = np.random.uniform(1 - 0.3, 0.9)\n    squeeze_y = np.random.uniform(1 - 0.3, 0.9)\n\n    pts2_p = generate_pts2(max_shift=0.2)\n\n    angle_rs = np.random.choice([-1, 1]) * np.random.uniform(10, 20)\n    squeeze_rx = np.random.uniform(1 - 0.3, 0.9)\n    squeeze_ry = np.random.uniform(1 - 0.3, 0.9)\n\n    angle_rt = np.random.choice([-1, 1]) * np.random.uniform(10, 20)\n    dx_rt = np.random.choice([-1, 1]) * np.random.uniform(0.05, 0.1)\n    dy_rt = np.random.choice([-1, 1]) * np.random.uniform(0.05, 0.1)\n\n    angle_rp = np.random.choice([-1, 1]) * np.random.uniform(10, 20)\n    pts2_rp = generate_pts2(max_shift=0.2)\n\n    dx_ts = np.random.choice([-1, 1]) * np.random.uniform(0.075, 0.15)\n    dy_ts = np.random.choice([-1, 1]) * np.random.uniform(0.075, 0.15)\n    squeeze_tsx = np.random.uniform(1 - 0.3, 0.9)\n    squeeze_tsy = np.random.uniform(1 - 0.3, 0.9)\n\n    dx_tp = np.random.choice([-1, 1]) * np.random.uniform(0.1, 0.25)\n    dy_tp = np.random.choice([-1, 1]) * np.random.uniform(0.1, 0.25)\n    pts2_tp = generate_pts2(max_shift=0.2)\n\n    pts2_ps = generate_pts2(max_shift=0.2)\n    squeeze_psx = np.random.uniform(1 - 0.3, 0.9)\n    squeeze_psy = np.random.uniform(1 - 0.3, 0.9)\n\n    for idx, landmarks in landmarks_dict.items():\n        rotated_landmarks_dict[idx] = rotate_landmarks(landmarks, angle_r, center)\n        squeezed_landmarks_dict[idx] = squeeze_landmarks(landmarks, squeeze_x, squeeze_y)  # <-- thay translated\n        perspectived_landmarks_dict[idx] = perspective_transform(landmarks, pts2_p)\n\n        rotated = rotate_landmarks(landmarks, angle_rs, center)\n        rotated_squeezed_landmarks_dict[idx] = squeeze_landmarks(rotated, squeeze_rx, squeeze_ry)\n\n        rotated2 = rotate_landmarks(landmarks, angle_rt, center)\n        rotated_translated_landmarks_dict[idx] = translate_landmarks(rotated2, dx_rt, dy_rt)\n\n        rotated3 = rotate_landmarks(landmarks, angle_rp, center)\n        rotated_perspective_landmarks_dict[idx] = perspective_transform(rotated3, pts2_rp)\n\n        translated = translate_landmarks(landmarks, dx_ts, dy_ts)\n        translated_squeezed_landmarks_dict[idx] = squeeze_landmarks(translated, squeeze_tsx, squeeze_tsy)\n\n        translated2 = translate_landmarks(landmarks, dx_tp, dy_tp)\n        translated_perspective_landmarks_dict[idx] = perspective_transform(translated2, pts2_tp)\n\n        perspectived = perspective_transform(landmarks, pts2_ps)\n        perspective_squeezed_landmarks_dict[idx] = squeeze_landmarks(perspectived, squeeze_psx, squeeze_psy)\n\n    return (\n        rotated_landmarks_dict,\n        squeezed_landmarks_dict,  \n        perspectived_landmarks_dict,\n        rotated_squeezed_landmarks_dict,\n        rotated_translated_landmarks_dict,\n        rotated_perspective_landmarks_dict,\n        translated_squeezed_landmarks_dict,\n        translated_perspective_landmarks_dict,\n        perspective_squeezed_landmarks_dict\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:32:25.734322Z","iopub.execute_input":"2025-10-19T14:32:25.734712Z","iopub.status.idle":"2025-10-19T14:32:25.749608Z","shell.execute_reply.started":"2025-10-19T14:32:25.734682Z","shell.execute_reply":"2025-10-19T14:32:25.748265Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def calculate_head_unit(pose_landmarks):\n\n    left_eye, right_eye = pose_landmarks[3], pose_landmarks[6]\n    head_unit = euclidean_distance(left_eye,right_eye)\n    return head_unit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:33:10.043366Z","iopub.execute_input":"2025-10-19T14:33:10.043725Z","iopub.status.idle":"2025-10-19T14:33:10.048810Z","shell.execute_reply.started":"2025-10-19T14:33:10.043696Z","shell.execute_reply":"2025-10-19T14:33:10.047242Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def calculate_sign_space(pose_landmarks):\n    head_unit = calculate_head_unit(pose_landmarks)\n\n    nose = pose_landmarks[0]\n   \n   \n    width = 7 * head_unit\n    \n    center_x, center_y , center_z = nose\n\n    x1 = center_x - width / 2\n\n    y1 = center_y - 1.5 * head_unit \n    x2 = center_x + width / 2\n    y2 = center_y + 8 * head_unit \n    return [x1, y1, x2, y2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:33:15.648836Z","iopub.execute_input":"2025-10-19T14:33:15.649253Z","iopub.status.idle":"2025-10-19T14:33:15.655350Z","shell.execute_reply.started":"2025-10-19T14:33:15.649208Z","shell.execute_reply":"2025-10-19T14:33:15.653974Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def draw_normalized_sign_space(frame, sign_space):\n    x1, y1, x2, y2 = sign_space\n    h, w, _ = frame.shape\n\n    # Convert normalized coordinates to pixel coordinates\n    x1_pixel, y1_pixel = int(x1 * w), int(y1 * h)\n    x2_pixel, y2_pixel = int(x2 * w), int(y2 * h)\n\n    annotated_frame = frame.copy()\n    cv2.rectangle(annotated_frame, (x1_pixel, y1_pixel), (x2_pixel, y2_pixel), (0, 255, 0), 2)\n\n    return annotated_frame","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:33:34.176820Z","iopub.execute_input":"2025-10-19T14:33:34.177368Z","iopub.status.idle":"2025-10-19T14:33:34.184145Z","shell.execute_reply.started":"2025-10-19T14:33:34.177326Z","shell.execute_reply":"2025-10-19T14:33:34.182639Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def calculate_all_sign_space(landmarks_dict):\n    sign_spaces = {}\n    for idx, landmarks_data in landmarks_dict.items():\n        sign_spaces[idx] = calculate_sign_space(landmarks_data[\"pose\"])\n    return sign_spaces","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:33:44.561096Z","iopub.execute_input":"2025-10-19T14:33:44.561492Z","iopub.status.idle":"2025-10-19T14:33:44.566893Z","shell.execute_reply.started":"2025-10-19T14:33:44.561465Z","shell.execute_reply":"2025-10-19T14:33:44.565508Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def normalize_landmarks_to_sign_space(landmarks_dict, sign_spaces):\n    \n    normalized = {}\n    for landmarks_idx, landmarks_data in landmarks_dict.items():\n        Xmin, Ymin, Xmax, Ymax = sign_spaces[landmarks_idx]\n        w = Xmax - Xmin\n        h = Ymax - Ymin\n        normalized_landmarks = {}\n        for part in [\"pose\", \"right\", \"left\"]:\n            processed_points = []\n\n            z_nose = None\n            if part == \"pose\" and len(landmarks_data[part]) > 0:\n                z_nose = landmarks_data[part][0][2]\n                if abs(z_nose)< 0.01:\n                    z_nose = None\n\n            for point in landmarks_data[part]:\n                x = float(point[0])\n                y = float(point[1])\n                z = float(point[2])\n                if x != 0.0 and y != 0.0:\n                   x = (x - Xmin) / w\n                   y = (y - Ymin) / h\n\n                if z_nose is not None and abs(z) > 0.0001:\n                    z = z / z_nose\n                processed_points.append((x, y, z))\n\n            \n            normalized_landmarks[part] = processed_points\n        normalized[landmarks_idx] = normalized_landmarks\n    return normalized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:33:54.165902Z","iopub.execute_input":"2025-10-19T14:33:54.166353Z","iopub.status.idle":"2025-10-19T14:33:54.175601Z","shell.execute_reply.started":"2025-10-19T14:33:54.166320Z","shell.execute_reply":"2025-10-19T14:33:54.173989Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:34:00.156647Z","iopub.execute_input":"2025-10-19T14:34:00.157025Z","iopub.status.idle":"2025-10-19T14:34:00.163249Z","shell.execute_reply.started":"2025-10-19T14:34:00.156996Z","shell.execute_reply":"2025-10-19T14:34:00.161300Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"with open(\"/kaggle/input/wlasl-processed/nslt_2000.json\", \"r\") as f:\n    dataset = json.load(f)\n\n\nMISSING_VIDEOS_PATH = f\"{PROCESSED_DATA_DIR}/missing_videos.txt\" \n\nallowed_gloss_ids = {\n    2, 6, 8, 15, 17, 24, 26, 27, 28, 29, 46, 60, 171, 1, 4, 5, 9, 10, 11, 12,\n    16, 19, 21, 22, 23, 32, 33, 35, 36, 38, 39, 41, 42, 48, 50, 51, 52, 53, 54,\n    58, 59, 62, 65, 66, 67, 75, 77, 82, 84, 85, 86, 88, 89, 100, 102, 108, 110,\n    112, 113, 116, 121, 123, 126, 128, 130, 132, 133, 136, 140, 141, 144, 146,\n    151, 164, 167, 168, 169, 174, 177, 182, 184, 199, 202, 206, 208, 210, 223,\n    225, 226, 232, 233, 236, 244, 246, 247, 249, 256, 261, 267, 269\n}\nallowed_gloss_ids = set(map(str, allowed_gloss_ids)) \n\n\nsplit_data = {\"train\": {}, \"val\": {}, \"test\": {}}\ncount =0\n\nwith open(MISSING_VIDEOS_PATH, \"w\") as missing_file:\n    for video_id, info in dataset.items():\n        subset = info[\"subset\"]  \n        gloss_id = str(info[\"action\"][0])  \n        \n        if gloss_id not in allowed_gloss_ids:\n            continue\n    \n        if gloss_id not in split_data[subset]:\n            split_data[subset][gloss_id] = []\n            \n        # kiểm tra video có tồn tại\n        primary_video_path = rf'/kaggle/input/wlasl-processed/videos/{video_id}.mp4'\n        backup_video_path = rf'/kaggle/input/wlasl2000-resized/wlasl-complete/videos/{video_id}.mp4'\n        if os.path.exists(primary_video_path):\n            split_data[subset][gloss_id].append(primary_video_path)\n        elif os.path.exists(backup_video_path):\n            split_data[subset][gloss_id].append(backup_video_path)\n        else:\n            missing_file.write(f\"{video_id}\\n\")\n            count += 1\n    \n\n\nfor subset in [\"train\", \"val\", \"test\"]:\n    for gloss_id in split_data[subset]:\n        split_data[subset][gloss_id].sort()\n\n    split_data[subset] = dict(sorted(split_data[subset].items(), key=lambda x: int(x[0])))\n\n\n\n\n\n\n\n    \n    filename = f\"{PROCESSED_DATA_DIR}/{subset}_100.json\"\n    with open(filename, \"w\") as f:\n        json.dump(split_data[subset], f, indent=4)\n\ntotal_train = sum(len(videos) for videos in split_data[\"train\"].values())\ntotal_val = sum(len(videos) for videos in split_data[\"val\"].values())\ntotal_test = sum(len(videos) for videos in split_data[\"test\"].values())\ntotal_samples = total_train + total_val + total_test\ntrain_ratio = (total_train / total_samples) * 100\nval_ratio = (total_val / total_samples) * 100\ntest_ratio = (total_test / total_samples) * 100\n\nprint(f\"Train ratio: {train_ratio:.2f}%\")\nprint(f\"Validation ratio: {val_ratio:.2f}%\")\nprint(f\"Test ratio: {test_ratio:.2f}%\")\n","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-10-19T14:37:29.916463Z","iopub.execute_input":"2025-10-19T14:37:29.916822Z","iopub.status.idle":"2025-10-19T14:37:31.397430Z","shell.execute_reply.started":"2025-10-19T14:37:29.916796Z","shell.execute_reply":"2025-10-19T14:37:31.395949Z"}},"outputs":[{"name":"stdout","text":"Train ratio: 69.76%\nValidation ratio: 17.24%\nTest ratio: 13.01%\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:37:41.759796Z","iopub.execute_input":"2025-10-19T14:37:41.760288Z","iopub.status.idle":"2025-10-19T14:37:41.771908Z","shell.execute_reply.started":"2025-10-19T14:37:41.760255Z","shell.execute_reply":"2025-10-19T14:37:41.770547Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"OUTPUT_DIR = f\"{PROCESSED_DATA_DIR}/train_by_gloss\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef process_gloss(gloss_id, video_list):\n    gloss_data = []\n    for video_path in video_list:\n        try:\n            keyframes = Extract_key_frames(video_path)\n            if len(keyframes) < 4:\n                continue\n            landmarks_dict = extract_landmarks(keyframes)\n\n            (rotated_landmarks_dict, \n             translated_landmarks_dict, \n             perspectived_landmarks_dict, \n             rotated_squeezed_landmarks_dict, \n             rotated_translated_landmarks_dict, \n             rotated_perspective_landmarks_dict, \n             translated_squeezed_landmarks_dict, \n             translated_perspective_landmarks_dict, \n             perspective_squeezed_landmarks_dict) = augment_data(landmarks_dict)\n                \n\n            for augmented_landmarks in [\n                landmarks_dict,\n                rotated_landmarks_dict,\n                translated_landmarks_dict,\n                perspectived_landmarks_dict,\n                rotated_squeezed_landmarks_dict,\n                rotated_translated_landmarks_dict,\n                rotated_perspective_landmarks_dict,\n                translated_squeezed_landmarks_dict,\n                translated_perspective_landmarks_dict,\n                perspective_squeezed_landmarks_dict\n            ]:\n                filtered = filter_and_interpolate_landmarks(augmented_landmarks)\n                sign_spaces = calculate_all_sign_space(filtered)\n                normalized = normalize_landmarks_to_sign_space(filtered, sign_spaces)\n\n                gloss_data.append({\n                    \"keyframes\": len(normalized),\n                    \"landmarks\": normalized\n                })\n        except Exception as e:\n            print(f\" Error processing video: {video_path} (Gloss: {gloss_id}) - {e}\")\n            continue\n\n    output_path = os.path.join(OUTPUT_DIR, f\"{gloss_id}.json\")\n    with open(output_path, \"w\") as f:\n        json.dump(gloss_data, f)\n    \n    return gloss_id\n\n\ndef process_train():\n    print(\" Start Processing train data\")\n\n    with open(f\"{PROCESSED_DATA_DIR}/train_100.json\", \"r\") as f:\n        train_data = json.load(f)\n\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        futures = []\n        for gloss_id, video_list in train_data.items():\n            futures.append(executor.submit(process_gloss, gloss_id, video_list))\n\n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing glosses\"):\n            future.result() \n\n    print(\"All glosses have been processed and saved to separate files successfully!\")\n\n    OUTPUT_JSON_PATH = f\"{PROCESSED_DATA_DIR}/wasl100_landmarks_train.json\"\n    all_data = {}\n    for filename in os.listdir(OUTPUT_DIR):\n        gloss_id = filename.replace(\".json\", \"\")\n        with open(os.path.join(OUTPUT_DIR, filename), \"r\") as f:\n            all_data[gloss_id] = json.load(f)\n\n    with open(OUTPUT_JSON_PATH, \"w\") as f:\n        json.dump(all_data, f)\n\n    print(f\"The data has been merged: {OUTPUT_JSON_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:40:29.214000Z","iopub.execute_input":"2025-10-19T14:40:29.214441Z","iopub.status.idle":"2025-10-19T14:40:29.227586Z","shell.execute_reply.started":"2025-10-19T14:40:29.214412Z","shell.execute_reply":"2025-10-19T14:40:29.226050Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"!lscpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:40:42.345620Z","iopub.execute_input":"2025-10-19T14:40:42.345976Z","iopub.status.idle":"2025-10-19T14:40:42.522956Z","shell.execute_reply.started":"2025-10-19T14:40:42.345949Z","shell.execute_reply":"2025-10-19T14:40:42.521083Z"}},"outputs":[{"name":"stdout","text":"Architecture:             x86_64\n  CPU op-mode(s):         32-bit, 64-bit\n  Address sizes:          46 bits physical, 48 bits virtual\n  Byte Order:             Little Endian\nCPU(s):                   4\n  On-line CPU(s) list:    0-3\nVendor ID:                GenuineIntel\n  Model name:             Intel(R) Xeon(R) CPU @ 2.20GHz\n    CPU family:           6\n    Model:                79\n    Thread(s) per core:   2\n    Core(s) per socket:   2\n    Socket(s):            1\n    Stepping:             0\n    BogoMIPS:             4399.99\n    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge m\n                          ca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht sysc\n                          all nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xt\n                          opology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq\n                           ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt\n                           aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dno\n                          wprefetch pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust\n                           bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx s\n                          map xsaveopt arat md_clear arch_capabilities\nVirtualization features:  \n  Hypervisor vendor:      KVM\n  Virtualization type:    full\nCaches (sum of all):      \n  L1d:                    64 KiB (2 instances)\n  L1i:                    64 KiB (2 instances)\n  L2:                     512 KiB (2 instances)\n  L3:                     55 MiB (1 instance)\nNUMA:                     \n  NUMA node(s):           1\n  NUMA node0 CPU(s):      0-3\nVulnerabilities:          \n  Gather data sampling:   Not affected\n  Itlb multihit:          Not affected\n  L1tf:                   Mitigation; PTE Inversion\n  Mds:                    Mitigation; Clear CPU buffers; SMT Host state unknown\n  Meltdown:               Mitigation; PTI\n  Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode;\n                           SMT Host state unknown\n  Reg file data sampling: Not affected\n  Retbleed:               Mitigation; IBRS\n  Spec rstack overflow:   Not affected\n  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prct\n                          l\n  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointe\n                          r sanitization\n  Spectre v2:             Mitigation; IBRS; IBPB conditional; STIBP conditional;\n                           RSB filling; PBRSB-eIBRS Not affected; BHI SW loop, K\n                          VM SW loop\n  Srbds:                  Not affected\n  Tsx async abort:        Mitigation; Clear CPU buffers; SMT Host state unknown\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"process_train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:41:58.561044Z","iopub.execute_input":"2025-10-19T14:41:58.561578Z","iopub.status.idle":"2025-10-19T15:07:53.395776Z","shell.execute_reply.started":"2025-10-19T14:41:58.561542Z","shell.execute_reply":"2025-10-19T15:07:53.394215Z"}},"outputs":[{"name":"stdout","text":" Start Processing traindata\n","output_type":"stream"},{"name":"stderr","text":"Processing glosses:  33%|███▎      | 33/100 [09:37<18:58, 17.00s/it] <ipython-input-26-3a6298fdce14>:23: RuntimeWarning: invalid value encountered in scalar divide\n  x = (x - Xmin) / w\n<ipython-input-26-3a6298fdce14>:24: RuntimeWarning: invalid value encountered in scalar divide\n  y = (y - Ymin) / h\n<ipython-input-26-3a6298fdce14>:23: RuntimeWarning: divide by zero encountered in scalar divide\n  x = (x - Xmin) / w\n<ipython-input-26-3a6298fdce14>:24: RuntimeWarning: divide by zero encountered in scalar divide\n  y = (y - Ymin) / h\nProcessing glosses: 100%|██████████| 100/100 [23:50<00:00, 14.31s/it]\n","output_type":"stream"},{"name":"stdout","text":"All glosses have been processed and saved to separate files successfully!\nThe data has been merged: /kaggle/working/wasl100_landmarks_train.json\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!rm -rf /kaggle/working/train_by_gloss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:08:01.868577Z","iopub.execute_input":"2025-10-19T15:08:01.868972Z","iopub.status.idle":"2025-10-19T15:08:02.231471Z","shell.execute_reply.started":"2025-10-19T15:08:01.868935Z","shell.execute_reply":"2025-10-19T15:08:02.229425Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def process_val():\n    print(\"Start Processing valdata\")\n    \n    OUTPUT_JSON_PATH = f\"{PROCESSED_DATA_DIR}/wasl100_landmarks_val.json\"\n    \n    with open(f\"{PROCESSED_DATA_DIR}/val_100.json\", \"r\") as f:\n        val_data = json.load(f)\n    \n    processed_data = {}\n    for gloss_id, video_list in tqdm(val_data.items(), desc=\"Val: Processing gloss IDs\"):\n        processed_data[gloss_id] = [] \n    \n        for video_path in video_list:\n            try:\n                keyframes = Extract_key_frames(video_path)\n                if len(keyframes) <4: #< 5:\n                    continue\n                landmarks_dict = extract_landmarks(keyframes)\n                filtered = filter_and_interpolate_landmarks(landmarks_dict)\n                sign_spaces = calculate_all_sign_space(filtered)\n                normalized = normalize_landmarks_to_sign_space(filtered, sign_spaces)\n                processed_data[gloss_id].append({\n                    \"keyframes\": len(normalized),\n                    \"landmarks\": normalized\n                    })\n            except Exception as e:\n                print(f\" Error processing video: {video_path} (Gloss: {gloss_id}) - {e}\")\n                continue\n            \n        with open(OUTPUT_JSON_PATH, \"w\") as f:\n            json.dump(processed_data, f, indent=None)\n    print(\"All videos have been processed and saved successfully! \"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:08:04.887274Z","iopub.execute_input":"2025-10-19T15:08:04.887768Z","iopub.status.idle":"2025-10-19T15:08:04.897609Z","shell.execute_reply.started":"2025-10-19T15:08:04.887729Z","shell.execute_reply":"2025-10-19T15:08:04.896277Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"> process_test","metadata":{}},{"cell_type":"code","source":"def process_test():\n    print(\"Start Processing testdata\")\n    \n    OUTPUT_JSON_PATH = f\"{PROCESSED_DATA_DIR}/wasl100_landmarks_test.json\"\n    \n    with open(f\"{PROCESSED_DATA_DIR}/test_100.json\", \"r\") as f:\n        test_data = json.load(f)\n    \n    processed_data = {}\n    for gloss_id, video_list in tqdm(test_data.items(), desc=\"test: Processing gloss IDs\"):\n        processed_data[gloss_id] = []\n    \n        for video_path in video_list:\n            try:\n                keyframes = Extract_key_frames(video_path)\n                if len(keyframes) < 4:\n                    continue\n                landmarks_dict = extract_landmarks(keyframes)\n                filtered = filter_and_interpolate_landmarks(landmarks_dict)\n                sign_spaces = calculate_all_sign_space(filtered)\n                normalized = normalize_landmarks_to_sign_space(filtered, sign_spaces)\n                processed_data[gloss_id].append({\n                    \"keyframes\": len(normalized),\n                    \"landmarks\": normalized\n                    })\n            except Exception as e:\n                print(f\" Error processing video: {video_path} (Gloss: {gloss_id}) - {e}\")\n                continue\n            \n        with open(OUTPUT_JSON_PATH, \"w\") as f:\n            json.dump(processed_data, f, indent=None)\n    print(\"All videos have been processed and saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:09:03.549604Z","iopub.execute_input":"2025-10-19T15:09:03.550153Z","iopub.status.idle":"2025-10-19T15:09:03.559864Z","shell.execute_reply.started":"2025-10-19T15:09:03.550110Z","shell.execute_reply":"2025-10-19T15:09:03.558348Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import concurrent.futures\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    futures = []\n    futures.append(executor.submit(process_test))\n    futures.append(executor.submit(process_val))\n\n    for future in concurrent.futures.as_completed(futures):\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:09:06.638662Z","iopub.execute_input":"2025-10-19T15:09:06.639032Z","iopub.status.idle":"2025-10-19T15:23:37.629759Z","shell.execute_reply.started":"2025-10-19T15:09:06.639005Z","shell.execute_reply":"2025-10-19T15:23:37.628349Z"}},"outputs":[{"name":"stdout","text":"Start Processing testdata\nStart Processing valdata\n","output_type":"stream"},{"name":"stderr","text":"test: Processing gloss IDs:   0%|          | 0/100 [00:00<?, ?it/s]\ntest: Processing gloss IDs:   1%|          | 1/100 [00:10<17:05, 10.36s/it]\ntest: Processing gloss IDs:   3%|▎         | 3/100 [00:28<14:38,  9.06s/it][A\ntest: Processing gloss IDs:   4%|▍         | 4/100 [00:33<11:55,  7.45s/it][A\ntest: Processing gloss IDs:   5%|▌         | 5/100 [00:42<13:06,  8.28s/it][A\ntest: Processing gloss IDs:   6%|▌         | 6/100 [00:50<12:21,  7.89s/it][A\ntest: Processing gloss IDs:   8%|▊         | 8/100 [01:02<10:36,  6.92s/it][A\ntest: Processing gloss IDs:   9%|▉         | 9/100 [01:10<10:48,  7.12s/it][A\ntest: Processing gloss IDs:  10%|█         | 10/100 [01:16<10:18,  6.87s/it]A\ntest: Processing gloss IDs:  12%|█▏        | 12/100 [01:30<10:13,  6.97s/it]A\ntest: Processing gloss IDs:  13%|█▎        | 13/100 [01:36<09:52,  6.81s/it]A\ntest: Processing gloss IDs:  15%|█▌        | 15/100 [01:49<09:23,  6.63s/it][A\ntest: Processing gloss IDs:  16%|█▌        | 16/100 [01:54<08:56,  6.39s/it][A\ntest: Processing gloss IDs:  17%|█▋        | 17/100 [02:04<10:06,  7.31s/it][A\ntest: Processing gloss IDs:  18%|█▊        | 18/100 [02:12<10:09,  7.44s/it][A\ntest: Processing gloss IDs:  20%|██        | 20/100 [02:25<09:07,  6.84s/it][A\ntest: Processing gloss IDs:  21%|██        | 21/100 [02:30<08:29,  6.45s/it][A\ntest: Processing gloss IDs:  24%|██▍       | 24/100 [02:44<06:47,  5.36s/it][A\ntest: Processing gloss IDs:  25%|██▌       | 25/100 [02:50<06:45,  5.40s/it][A\ntest: Processing gloss IDs:  26%|██▌       | 26/100 [02:59<08:16,  6.71s/it][A\ntest: Processing gloss IDs:  28%|██▊       | 28/100 [03:11<07:16,  6.06s/it][A\ntest: Processing gloss IDs:  30%|███       | 30/100 [03:23<07:00,  6.01s/it][A\ntest: Processing gloss IDs:  31%|███       | 31/100 [03:30<07:25,  6.46s/it][A\ntest: Processing gloss IDs:  32%|███▏      | 32/100 [03:37<07:35,  6.70s/it][A\ntest: Processing gloss IDs:  34%|███▍      | 34/100 [03:48<06:33,  5.96s/it][A\ntest: Processing gloss IDs:  35%|███▌      | 35/100 [03:52<05:51,  5.41s/it][A\ntest: Processing gloss IDs:  37%|███▋      | 37/100 [04:03<05:35,  5.32s/it][A\ntest: Processing gloss IDs:  38%|███▊      | 38/100 [04:08<05:24,  5.24s/it][A\ntest: Processing gloss IDs:  40%|████      | 40/100 [04:23<06:19,  6.33s/it][A\ntest: Processing gloss IDs:  41%|████      | 41/100 [04:28<05:50,  5.93s/it][A\ntest: Processing gloss IDs:  42%|████▏     | 42/100 [04:37<06:31,  6.75s/it][A\ntest: Processing gloss IDs:  44%|████▍     | 44/100 [04:48<05:48,  6.22s/it][A\ntest: Processing gloss IDs:  45%|████▌     | 45/100 [04:54<05:32,  6.04s/it][A\ntest: Processing gloss IDs:  46%|████▌     | 46/100 [05:03<06:10,  6.86s/it][A\ntest: Processing gloss IDs:  47%|████▋     | 47/100 [05:10<06:17,  7.12s/it][A\ntest: Processing gloss IDs:  48%|████▊     | 48/100 [05:17<06:06,  7.05s/it][A\ntest: Processing gloss IDs:  49%|████▉     | 49/100 [05:27<06:38,  7.82s/it][A\ntest: Processing gloss IDs:  50%|█████     | 50/100 [05:37<07:09,  8.58s/it][A\ntest: Processing gloss IDs:  51%|█████     | 51/100 [05:47<07:21,  9.01s/it][A\ntest: Processing gloss IDs:  52%|█████▏    | 52/100 [05:53<06:22,  7.97s/it][A\ntest: Processing gloss IDs:  55%|█████▌    | 55/100 [06:09<04:42,  6.28s/it][A\ntest: Processing gloss IDs:  57%|█████▋    | 57/100 [06:20<04:05,  5.71s/it][A\ntest: Processing gloss IDs:  58%|█████▊    | 58/100 [06:28<04:27,  6.37s/it][A\ntest: Processing gloss IDs:  59%|█████▉    | 59/100 [06:34<04:19,  6.32s/it][A\ntest: Processing gloss IDs:  60%|██████    | 60/100 [06:42<04:30,  6.77s/it][A\ntest: Processing gloss IDs:  61%|██████    | 61/100 [06:51<04:57,  7.62s/it][A\nVal: Processing gloss IDs:  45%|████▌     | 45/100 [06:54<08:00,  8.74s/it]\u001b[A\ntest: Processing gloss IDs:  63%|██████▎   | 63/100 [07:07<04:36,  7.46s/it][A\ntest: Processing gloss IDs:  64%|██████▍   | 64/100 [07:14<04:23,  7.33s/it][A\ntest: Processing gloss IDs:  65%|██████▌   | 65/100 [07:20<04:07,  7.08s/it][A\ntest: Processing gloss IDs:  66%|██████▌   | 66/100 [07:28<04:04,  7.19s/it][A","output_type":"stream"},{"name":"stdout","text":" Error processing video: /kaggle/input/wlasl2000-resized/wlasl-complete/videos/28206.mp4 (Gloss: 86) - list index out of range\n","output_type":"stream"},{"name":"stderr","text":"\ntest: Processing gloss IDs:  67%|██████▋   | 67/100 [07:37<04:22,  7.96s/it][A\ntest: Processing gloss IDs:  68%|██████▊   | 68/100 [07:44<03:59,  7.48s/it][A\ntest: Processing gloss IDs:  70%|███████   | 70/100 [08:00<03:54,  7.81s/it][A\ntest: Processing gloss IDs:  72%|███████▏  | 72/100 [08:11<03:05,  6.62s/it][A\ntest: Processing gloss IDs:  73%|███████▎  | 73/100 [08:18<03:03,  6.80s/it][A\ntest: Processing gloss IDs:  74%|███████▍  | 74/100 [08:25<02:53,  6.66s/it][A\ntest: Processing gloss IDs:  76%|███████▌  | 76/100 [08:39<02:44,  6.85s/it][A\ntest: Processing gloss IDs:  77%|███████▋  | 77/100 [08:48<02:57,  7.72s/it][A\ntest: Processing gloss IDs:  78%|███████▊  | 78/100 [08:55<02:45,  7.52s/it][A\nVal: Processing gloss IDs:  59%|█████▉    | 59/100 [08:59<06:18,  9.23s/it]\u001b[A\ntest: Processing gloss IDs:  79%|███████▉  | 79/100 [09:04<02:42,  7.76s/it][A\ntest: Processing gloss IDs:  81%|████████  | 81/100 [09:20<02:26,  7.71s/it][A\ntest: Processing gloss IDs:  82%|████████▏ | 82/100 [09:29<02:28,  8.24s/it][A\ntest: Processing gloss IDs:  83%|████████▎ | 83/100 [09:37<02:19,  8.20s/it][A\ntest: Processing gloss IDs:  84%|████████▍ | 84/100 [09:45<02:09,  8.06s/it][A\ntest: Processing gloss IDs:  86%|████████▌ | 86/100 [10:01<01:49,  7.80s/it][A\ntest: Processing gloss IDs:  87%|████████▋ | 87/100 [10:09<01:44,  8.02s/it][A\ntest: Processing gloss IDs:  88%|████████▊ | 88/100 [10:18<01:38,  8.24s/it][A\ntest: Processing gloss IDs:  89%|████████▉ | 89/100 [10:25<01:25,  7.79s/it][A\ntest: Processing gloss IDs:  91%|█████████ | 91/100 [10:39<01:07,  7.47s/it][A\ntest: Processing gloss IDs:  92%|█████████▏| 92/100 [10:47<01:00,  7.60s/it][A\ntest: Processing gloss IDs:  94%|█████████▍| 94/100 [11:02<00:44,  7.48s/it][A\ntest: Processing gloss IDs:  95%|█████████▌| 95/100 [11:12<00:41,  8.28s/it][A\ntest: Processing gloss IDs:  96%|█████████▌| 96/100 [11:20<00:32,  8.13s/it][A\ntest: Processing gloss IDs:  98%|█████████▊| 98/100 [11:34<00:14,  7.44s/it][A\ntest: Processing gloss IDs:  99%|█████████▉| 99/100 [11:41<00:07,  7.49s/it][A\ntest: Processing gloss IDs: 100%|██████████| 100/100 [11:50<00:00,  7.11s/it]A\n","output_type":"stream"},{"name":"stdout","text":"All videos have been processed and saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"\nVal: Processing gloss IDs:  77%|███████▋  | 77/100 [11:54<03:43,  9.71s/it]\u001b[A\nVal: Processing gloss IDs:  78%|███████▊  | 78/100 [12:01<03:17,  8.98s/it]\u001b[A\nVal: Processing gloss IDs:  79%|███████▉  | 79/100 [12:09<03:05,  8.84s/it]\u001b[A\nVal: Processing gloss IDs:  80%|████████  | 80/100 [12:17<02:50,  8.51s/it]\u001b[A\nVal: Processing gloss IDs:  81%|████████  | 81/100 [12:26<02:44,  8.64s/it]\u001b[A\nVal: Processing gloss IDs:  82%|████████▏ | 82/100 [12:33<02:27,  8.19s/it]\u001b[A\nVal: Processing gloss IDs:  83%|████████▎ | 83/100 [12:41<02:16,  8.06s/it]\u001b[A\nVal: Processing gloss IDs:  84%|████████▍ | 84/100 [12:48<02:05,  7.82s/it]\u001b[A\nVal: Processing gloss IDs:  85%|████████▌ | 85/100 [12:55<01:53,  7.56s/it]\u001b[A\nVal: Processing gloss IDs:  86%|████████▌ | 86/100 [13:02<01:42,  7.30s/it]\u001b[A\nVal: Processing gloss IDs:  87%|████████▋ | 87/100 [13:10<01:37,  7.48s/it]\u001b[A\nVal: Processing gloss IDs:  88%|████████▊ | 88/100 [13:15<01:21,  6.78s/it]\u001b[A\nVal: Processing gloss IDs:  89%|████████▉ | 89/100 [13:22<01:16,  6.95s/it]\u001b[A\nVal: Processing gloss IDs:  90%|█████████ | 90/100 [13:30<01:12,  7.29s/it]\u001b[A\nVal: Processing gloss IDs:  91%|█████████ | 91/100 [13:37<01:04,  7.15s/it]\u001b[A\nVal: Processing gloss IDs:  92%|█████████▏| 92/100 [13:45<00:59,  7.43s/it]\u001b[A\nVal: Processing gloss IDs:  93%|█████████▎| 93/100 [13:52<00:49,  7.09s/it]\u001b[A\nVal: Processing gloss IDs:  94%|█████████▍| 94/100 [13:57<00:40,  6.69s/it]\u001b[A\nVal: Processing gloss IDs:  95%|█████████▌| 95/100 [14:03<00:31,  6.37s/it]\u001b[A\nVal: Processing gloss IDs:  96%|█████████▌| 96/100 [14:09<00:25,  6.38s/it]\u001b[A\nVal: Processing gloss IDs:  97%|█████████▋| 97/100 [14:15<00:18,  6.07s/it]\u001b[A\nVal: Processing gloss IDs:  98%|█████████▊| 98/100 [14:20<00:11,  5.86s/it]\u001b[A\nVal: Processing gloss IDs:  99%|█████████▉| 99/100 [14:25<00:05,  5.61s/it]\u001b[A\nVal: Processing gloss IDs: 100%|██████████| 100/100 [14:30<00:00,  8.71s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"All videos have been processed and saved successfully! \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"input_file_path = \"/kaggle/input/wlasl-processed/wlasl_class_list.txt\"\noutput_file_path = \"/kaggle/working/top_100_classes.txt\"\n\n\n\ntry:\n    with open(input_file_path, \"r\") as input_file:\n        with open(output_file_path, \"w\") as output_file:\n            for i, line in enumerate(input_file):\n                if str(i) in allowed_gloss_ids:\n                    output_file.write(line)\n    print(f\"Saved {output_file_path}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:26:02.386258Z","iopub.execute_input":"2025-10-19T15:26:02.386771Z","iopub.status.idle":"2025-10-19T15:26:02.409291Z","shell.execute_reply.started":"2025-10-19T15:26:02.386736Z","shell.execute_reply":"2025-10-19T15:26:02.406980Z"}},"outputs":[{"name":"stdout","text":"Saved /kaggle/working/top_100_classes.txt\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}